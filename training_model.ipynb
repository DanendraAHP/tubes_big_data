{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "VERBOSE = 1\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data 1\n",
    "df_1 = pd.read_csv('data_analisis_sentimen_1.csv', sep=';')[['tweet', 'hasil']].dropna()\n",
    "#data 2\n",
    "df_2 = pd.read_csv('data_analisis_sentimen_2.csv', sep=';')[['Tweet', 'hasil']].dropna().rename(columns={'Tweet':'tweet'})\n",
    "#data 4\n",
    "df_3 = pd.read_csv('data_analisis_sentimen_4.csv', sep=';')[['content', 'Hasil']].dropna().rename(columns={'content':'tweet', 'Hasil':'hasil'})\n",
    "df = pd.concat([df_1, df_2, df_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    10792\n",
       "2.0     8535\n",
       "0.0     7610\n",
       "Name: hasil, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['hasil']<=1]\n",
    "df['hasil'] = df['hasil'].apply(lambda x:x+1)\n",
    "# 0 = negatif, 1 = netral, 2=positif\n",
    "df['hasil'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df['tweet'], df['hasil']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "y_train = tf.one_hot(y_train, 3)\n",
    "y_test = tf.one_hot(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21549,), (5388,), TensorShape([21549, 3]), TensorShape([5388, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x_tr, x_val, load, tokenizer_file):\n",
    "    #Tokenize the sentences\n",
    "    if load:\n",
    "        with open(tokenizer_file, 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "    elif not load:\n",
    "        tokenizer = Tokenizer(num_words=50000)\n",
    "        tokenizer.fit_on_texts(list(x_tr))\n",
    "    #converting text into integer sequences\n",
    "    x_tr_seq  = tokenizer.texts_to_sequences(x_tr) \n",
    "    x_val_seq = tokenizer.texts_to_sequences(x_val)\n",
    "    #padding to prepare sequences of same length\n",
    "    x_tr_seq  = pad_sequences(x_tr_seq, maxlen=100)\n",
    "    x_val_seq = pad_sequences(x_val_seq, maxlen=100)\n",
    "    return x_tr_seq, x_val_seq, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer, tokenizer_file):\n",
    "    with open(tokenizer_file, 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_test, tokenizer = tokenize(x_train, x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(vocab_size, embedding_dim, architecture):\n",
    "    \"\"\"\n",
    "    For baseline model comparison only\n",
    "    \"\"\"\n",
    "    model=tf.keras.Sequential()\n",
    "    #embedding layer\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim)) \n",
    "    if architecture=='LSTM':\n",
    "        #lstm layer\n",
    "        model.add(tf.keras.layers.LSTM(128,return_sequences=True,dropout=0.2))\n",
    "        #Global Maxpooling\n",
    "        model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    elif architecture=='BI-LSTM':\n",
    "        #lstm layer\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True,dropout=0.2)))\n",
    "        #Global Maxpooling\n",
    "        model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    #Dense Layer\n",
    "    model.add(tf.keras.layers.Dense(64,activation='relu')) \n",
    "    model.add(tf.keras.layers.Dense(3,activation='softmax')) \n",
    "    #Add loss function, metrics, optimizer\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=[\"acc\"]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_tr, y_tr, x_val, y_val, model_file, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE):\n",
    "    #Adding callbacks\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=verbose,patience=5)  \n",
    "    mc=tf.keras.callbacks.ModelCheckpoint(model_file, monitor='val_acc', mode='max', save_best_only=True,verbose=verbose)  \n",
    "    history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=batch_size,epochs=epochs,validation_data=(np.array(x_val),np.array(y_val)),verbose=verbose,callbacks=[es,mc])\n",
    "    model.save(model_file)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(x_val, y_val, model):\n",
    "    predict = model.predict(x_val)\n",
    "    y_pred = np.argmax(predict, axis=1)\n",
    "    y_true = np.argmax(y_val, axis=1)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(x_tr, x_test, y_tr, y_test, **args):\n",
    "    \"\"\"\n",
    "    kalau reduced false artinya pakai autoencoder dulu asumsinya\n",
    "    \"\"\"\n",
    "    #tokenize\n",
    "    print(\"------------Now tokenizing data------------\")\n",
    "    x_tr, x_test, tokenizer = tokenize(x_tr, x_test, args['load_tokenizer'], args['tokenizer_file'])\n",
    "    print(\"------------Tokenizing data done------------\")\n",
    "    #create model\n",
    "    print(\"------------Now creating and training model------------\")\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    model = init_model(vocab_size, EMBEDDING_DIM, args['architecture'])\n",
    "    #train model\n",
    "    train_model(model, x_tr, y_tr, x_test, y_test, args['model_file'])\n",
    "    print(\"------------Training model done------------\")\n",
    "    #evaluate model\n",
    "    print(\"------------Model evaluation------------\")\n",
    "    eval_model(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Now tokenizing data------------\n",
      "------------Tokenizing data done------------\n",
      "------------Now creating and training model------------\n",
      "Epoch 1/50\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.9256 - acc: 0.5423\n",
      "Epoch 1: val_acc improved from -inf to 0.66722, saving model to model\\test_lstm.h5\n",
      "169/169 [==============================] - 5s 23ms/step - loss: 0.9256 - acc: 0.5423 - val_loss: 0.7552 - val_acc: 0.6672\n",
      "Epoch 2/50\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.6309 - acc: 0.7358\n",
      "Epoch 2: val_acc improved from 0.66722 to 0.68114, saving model to model\\test_lstm.h5\n",
      "169/169 [==============================] - 4s 21ms/step - loss: 0.6309 - acc: 0.7358 - val_loss: 0.7437 - val_acc: 0.6811\n",
      "Epoch 3/50\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8402\n",
      "Epoch 3: val_acc did not improve from 0.68114\n",
      "169/169 [==============================] - 3s 18ms/step - loss: 0.4245 - acc: 0.8400 - val_loss: 0.8781 - val_acc: 0.6611\n",
      "Epoch 4/50\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.2680 - acc: 0.9041\n",
      "Epoch 4: val_acc did not improve from 0.68114\n",
      "169/169 [==============================] - 3s 19ms/step - loss: 0.2680 - acc: 0.9041 - val_loss: 0.9702 - val_acc: 0.6665\n",
      "Epoch 5/50\n",
      "166/169 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9448\n",
      "Epoch 5: val_acc did not improve from 0.68114\n",
      "169/169 [==============================] - 3s 20ms/step - loss: 0.1616 - acc: 0.9448 - val_loss: 1.1918 - val_acc: 0.6487\n",
      "Epoch 6/50\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.1124 - acc: 0.9633\n",
      "Epoch 6: val_acc did not improve from 0.68114\n",
      "169/169 [==============================] - 3s 19ms/step - loss: 0.1124 - acc: 0.9633 - val_loss: 1.3811 - val_acc: 0.6379\n",
      "Epoch 7/50\n",
      "169/169 [==============================] - ETA: 0s - loss: 0.0775 - acc: 0.9741\n",
      "Epoch 7: val_acc did not improve from 0.68114\n",
      "169/169 [==============================] - 3s 18ms/step - loss: 0.0775 - acc: 0.9741 - val_loss: 1.5345 - val_acc: 0.6366\n",
      "Epoch 7: early stopping\n",
      "------------Training model done------------\n",
      "------------Model evaluation------------\n",
      "169/169 [==============================] - 1s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.60      0.62      1539\n",
      "           1       0.61      0.66      0.64      2143\n",
      "           2       0.67      0.64      0.65      1706\n",
      "\n",
      "    accuracy                           0.64      5388\n",
      "   macro avg       0.64      0.63      0.64      5388\n",
      "weighted avg       0.64      0.64      0.64      5388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(x_train, x_test, y_train, y_test, architecture='LSTM', model_file='model/test_lstm.h5', tokenizer_file ='model/tokenizer.pickle', load_tokenizer=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4179a95aa5b49abc81a1f9774c9389b2d4eba524b70fc2f22d886502bea2eb9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
