{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the inference from HDFS to HDFS in parquet format file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 10:11:43.730477: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_utils import Dataset\n",
    "from src.model import TFModel\n",
    "from datetime import datetime\n",
    "from config.config import INFERENCE_CONFIG\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#create spark \n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "#preprocess before inference\n",
    "RAW_FOLDER = INFERENCE_CONFIG['RAW_FOLDER']\n",
    "RAW_FOLDER = f'{RAW_FOLDER}/*.parquet'\n",
    "df = spark.read.parquet(RAW_FOLDER)\n",
    "pd_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd_df.rename(columns={\n",
    "    '_1' : 'video_id',\n",
    "    '_2' : 'description',\n",
    "    '_3' : 'published_at',\n",
    "    '_4' : 'channel_name',\n",
    "    '_5' : 'video_title',\n",
    "    '_6' : 'figure',\n",
    "    '_7' : 'keyword'\n",
    "})\n",
    "pd_df = pd_df.drop_duplicates(subset='video_id')\n",
    "pd_df = pd_df.dropna(subset=['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.to_csv(INFERENCE_CONFIG['RAW_CSV_FILE'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------now creating dataset--------------------\n"
     ]
    }
   ],
   "source": [
    "#model inference\n",
    "train = False\n",
    "#prepare the inference data\n",
    "print('now creating dataset'.center(60, '-'))\n",
    "data = Dataset(train)\n",
    "data.prepare_inference_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 1s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "model = TFModel()\n",
    "y = model.inference(data.tfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "data.df['result'] = y\n",
    "data.df['inference_date'] = dt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "sparkDF=spark.createDataFrame(data.df) \n",
    "sparkDF.write.save(INFERENCE_CONFIG['INFERENCE_FOLDER'], format='parquet', mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- published_at: string (nullable = true)\n",
      " |-- channel_name: string (nullable = true)\n",
      " |-- video_title: string (nullable = true)\n",
      " |-- sentiment: long (nullable = true)\n",
      "\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|   video_id|         description|        published_at|        channel_name|         video_title|sentiment|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|poZtdyC24P4|Tutorial belajar ...|2023-02-24T23:45:01Z|         Dea Afrizal|Belajar Dasar Pem...|        0|\n",
      "|rfscVS0vtbw|This course will ...|2018-07-11T18:00:42Z|    freeCodeCamp.org|Learn Python - Fu...|        1|\n",
      "|xETkm9H6aaY|Yuk Belajar Pytho...|2023-02-13T10:00:36Z|       Kelas Terbuka|Belajar Python [D...|        2|\n",
      "|_uQrJ0TkZlc|Python tutorial -...|2019-02-18T15:00:08Z|Programming with ...|Python Tutorial -...|        1|\n",
      "|kqtD5dpn9C8|This Python tutor...|2020-09-16T13:00:20Z|Programming with ...|Python for Beginn...|        1|\n",
      "|rWC2iFlN3TM|Halo teman-teman,...|2021-02-03T04:59:33Z|      Agung Setiawan|Tutorial Python B...|        2|\n",
      "|x7X9w_GIm1s|Python is arguabl...|2021-10-25T15:19:28Z|            Fireship|Python in 100 Sec...|        0|\n",
      "|NHquaDS6XQ0|99% of Python pro...|2022-09-29T23:51:42Z|            Indently|99% Of Python Pro...|        1|\n",
      "|G18YEjpwHDY|Tutorial belajar ...|2022-06-21T06:45:00Z|         Dea Afrizal|Tutorial Implemen...|        2|\n",
      "|XKHEtdqhLK8|Python tutorial f...|2021-02-15T15:00:12Z|            Bro Code|Python Full Cours...|        1|\n",
      "|-auWrbiaoGc|Yuk Belajar Pytho...|2020-04-20T03:00:02Z|       Kelas Terbuka|Belajar Python [D...|        2|\n",
      "|OaeHnwnlDqw|It's been six mon...|2021-04-16T11:59:48Z|        Nat Geo WILD|Feeding a Reticul...|        0|\n",
      "|t8pPdKYpowI|Python Tutorial f...|2021-03-05T14:10:17Z| TechWorld with Nana|Python Tutorial f...|        1|\n",
      "|E8NijUYfyus|Get better at Pyt...|2023-03-14T01:30:53Z|             mCoding|21 MORE nooby Pyt...|        1|\n",
      "|qecsnAXtms4|Today, I will be ...|2023-03-13T14:00:35Z|       Tech With Tim|The Truth About L...|        1|\n",
      "|b093aqAZiPU|In this step-by-s...|2021-03-25T10:00:08Z|     Kevin Stratvert|üë©‚Äçüíª Python for ...|        0|\n",
      "|ZkFKF-ohsKk|Get notified of t...|2023-03-07T04:26:42Z|    Nicholas Renotte|How to use the Ch...|        1|\n",
      "|aPhbz4fER0o|‚òÖ Gabung Menjadi ...|2022-02-24T13:00:11Z|YOUKU  Indonesia-...|INDO SUB (Snake I...|        2|\n",
      "|eWRfhZUzrAc|Learn the Python ...|2022-08-09T12:54:23Z|    freeCodeCamp.org|Python for Beginn...|        1|\n",
      "|bVc8kqh8dKk|üöÄ Become a Real ...|2023-02-01T13:00:33Z| Internet Made Coder|How to MASTER Pyt...|        1|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test hasil loading inference data\n",
    "inference_folder = INFERENCE_CONFIG['INFERENCE_FOLDER']\n",
    "load_df = spark.read.parquet(f'{inference_folder}/*.parquet')\n",
    "load_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
